# app/handlers.py

import json
import os
import tempfile
import whisper
import ollama
from aiogram import Router, F
from aiogram.filters import Command
from aiogram.types import (
    Message, CallbackQuery, BufferedInputFile,
    InlineKeyboardMarkup, InlineKeyboardButton
)
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import StatesGroup, State

from app.config import OPENROUTER_API_KEY
from app.keyboards.start_keyboard import lang_menu, start_kb, back_to_start
from app.static_files.bot_answers import GREETINGS, CERTIFICATE
from app.new_voice_handler import chat_lang, WHISPER_LANG
from app.workTools.WorkWithDB import WorkWithDB
from app.workTools.WorkWithTTS import WorkWithTTS
from app.workTools.WorkWithLLM import MistralAPI
from app.workTools.WorkWithCache import WorkWithCache
from app.workTools.search_db import search_db

# FSM states
class Flag(StatesGroup):
    awaiting_question = State()
    awaiting_tts_text = State()
    awaiting_compare_selection = State()

router = Router()
_whisper_model = None

def get_whisper_model():
    global _whisper_model
    if _whisper_model is None:
        _whisper_model = whisper.load_model('tiny')
    return _whisper_model

# -------------------
# /start –∏ –≤—ã–±–æ—Ä —è–∑—ã–∫–∞
# -------------------
@router.message(Command('start'))
async def cmd_start(msg: Message):
    chat_lang.pop(msg.chat.id, None)
    await msg.answer(GREETINGS, reply_markup=lang_menu)

@router.callback_query(F.data.startswith('set_lang:'))
async def set_lang(c: CallbackQuery):
    lang = c.data.split(':', 1)[1]
    chat_lang[c.message.chat.id] = lang
    confirm = {'ru':'‚úÖ –†—É—Å—Å–∫–∏–π','en':'‚úÖ English','cn':'‚úÖ ‰∏≠Êñá'}[lang]
    await c.message.edit_text(confirm, reply_markup=await start_kb(lang))

# -------------------
# –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è / –ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è
# -------------------
@router.callback_query(F.data == 'performance')
@router.callback_query(F.data.startswith("presentaion_"))
async def show_intro(c: CallbackQuery):
    lang = chat_lang.get(c.message.chat.id, 'ru')
    key = f"{c.data}_{lang}"
    if WorkWithCache.check_key(key):
        audio_bytes, text = WorkWithCache.get_cache(key)
    else:
        drone = None
        if c.data == 'performance':
            template = {
                'ru': "–î–∞–π—Ç–µ –∫—Ä–∞—Ç–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ VTOL-–¥—Ä–æ–Ω–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–∏–π.",
                'en': "Provide a short translation of the VTOL drone presentation into English.",
                'cn': "ËØ∑Â∞ÜVTOLÊó†‰∫∫Êú∫ÁöÑ‰ªãÁªçÁÆÄÁü≠Âú∞ÁøªËØëÊàê‰∏≠Êñá„ÄÇ"
            }[lang]
        else:
            drone = c.data.split('_',1)[1]
            template = {
                'ru': f"–°–æ–∫—Ä–∞—Ç–∏—Ç–µ –¥–æ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –æ–ø–∏—Å–∞–Ω–∏–µ –¥—Ä–æ–Ω–∞ {drone} –Ω–∞ —Ä—É—Å—Å–∫–æ–º.",
                'en': f"Summarize in 2 sentences a description of {drone} in English.",
                'cn': f"Áî®2Âè•ËØùÁÆÄË¶ÅÊèèËø∞{drone}„ÄÇ"
            }[lang]

        text = await MistralAPI.query(
            prompt=template,
            system=f"–í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –¥–ª—è {lang}-–∫–ª–∏–µ–Ω—Ç–∞.",
            max_tokens=100
        )
        audio_bytes = await WorkWithTTS.text_to_speech(task=key, text=text, lang=lang)
        WorkWithCache.append_cache(key, audio_bytes, text)

    await c.message.answer(text)
    await c.message.answer_audio(BufferedInputFile(audio_bytes, filename="intro.mp3"))

# -------------------
# –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: —Å–ø–∏—Å–æ–∫ –¥—Ä–æ–Ω–æ–≤ –∏ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
# -------------------
@router.callback_query(F.data == 'features')
async def features_list(c: CallbackQuery):
    names = list(WorkWithDB.load_all().keys())
    buttons = [InlineKeyboardButton(text=n, callback_data=f"feat:{n}") for n in names]
    kb = InlineKeyboardMarkup(inline_keyboard=[buttons[i:i+2] for i in range(0, len(buttons), 2)])
    await c.message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫:", reply_markup=kb)

@router.callback_query(F.data.startswith("feat:"))
async def show_features(c: CallbackQuery):
    name = c.data.split(":",1)[1]
    specs = WorkWithDB.show_characteristics(name)
    lines = [f"üìå <b>{name}</b>"]
    for section in ("performance", "weights", "dimensions"):
        data = specs.get(section, {})
        if data:
            lines.append(f"\n<b>{section.title()}:</b>")
            for k,v in data.items():
                lines.append(f"‚Ä¢ {k}: {v}")
    docs = specs.get("compliance_documents", [])
    if docs:
        lines.append("\n<b>–î–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è:</b>")
        for d in docs:
            lines.append(f"‚Ä¢ {d}")

    text = "\n".join(lines)
    await c.message.answer(text, parse_mode="HTML")

# -------------------
# –°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã
# -------------------
@router.callback_query(F.data == 'certificate')
async def show_cert(c: CallbackQuery):
    lang = chat_lang.get(c.message.chat.id, 'ru')
    cert = CERTIFICATE[lang]
    docs = WorkWithDB.show_characteristics('JOUAV CW-15').get('compliance_documents', [])
    await c.message.answer(f"{cert}\n" + "\n".join(docs))

# -------------------
# –í—Ö–æ–¥ –≤ Q&A
# -------------------
@router.callback_query(F.data == 'question')
async def enter_qa(c: CallbackQuery, state: FSMContext):
    await state.set_state(Flag.awaiting_question)
    lang = chat_lang.get(c.message.chat.id, 'ru')
    prompt_text = {
        'ru': '‚ùì –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.',
        'en': '‚ùì Ask your question by text or send a voice message.',
        'cn': '‚ùì ËØ∑‰ª•ÊñáÂ≠óÊèêÈóÆÊàñÂèëÈÄÅËØ≠Èü≥Ê∂àÊÅØ„ÄÇ'
    }[lang]
    await c.message.answer(prompt_text)

@router.message(Flag.awaiting_question)
async def handle_question(m: Message, state: FSMContext):
    await state.clear()
    lang = chat_lang.get(m.chat.id, 'ru')

    if m.voice:
        with tempfile.NamedTemporaryFile(suffix=".ogg", delete=False) as tmp:
            await m.bot.download(m.voice.file_id, tmp.name)
            audio_path = tmp.name
        try:
            res = get_whisper_model().transcribe(audio_path, language=WHISPER_LANG.get(lang, 'en'))
            user_question = res.get('text', '').strip()
        except:
            user_question = ''
        finally:
            try: os.remove(audio_path)
            except: pass

        if not user_question:
            return await m.answer({
                'ru': "‚ùóÔ∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å.",
                'en': "‚ùóÔ∏è Could not transcribe audio.",
                'cn': "‚ùóÔ∏è Êó†Ê≥ïËØÜÂà´ËØ≠Èü≥„ÄÇ"
            }[lang])
    else:
        user_question = m.text or ""

    relevant = search_db(user_question, top_k=2)
    context = "\n\n".join([f"{name}:\n{info}" for name, info in relevant])
    system_prompt = {
        'ru': "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ VTOL-–¥—Ä–æ–Ω–∞–º. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞.",
        'en': "You are a VTOL drone expert. Use only the provided context to answer.",
        'cn': "ÊÇ®ÊòØ VTOL Êó†‰∫∫Êú∫‰∏ìÂÆ∂„ÄÇ‰ªÖ‰ΩøÁî®‰ª•‰∏ã‰ø°ÊÅØËøõË°åÂõûÁ≠î„ÄÇ"
    }[lang]

    prompt = f"{system_prompt}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {user_question}"

    answer = await MistralAPI.query(prompt=prompt, system=system_prompt, max_tokens=200)
    answer = answer.strip()
    if len(answer) > 1000:
        answer = answer[:997] + "..."

    await m.answer(answer)
    audio = await WorkWithTTS.text_to_speech(task="answer-question", text=answer, lang=lang)
    await m.answer_audio(BufferedInputFile(audio, filename="answer.mp3"))

# -------------------
# –ö–æ–º–ø–∞—Ä–∞—Ç–æ—Ä: –º—É–ª—å—Ç–∏–≤—ã–±–æ—Ä –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
# -------------------
@router.callback_query(F.data == 'compare')
async def ask_compare(c: CallbackQuery, state: FSMContext):
    # –ù–∞—á–∏–Ω–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä
    await state.set_state(Flag.awaiting_compare_selection)
    await state.update_data(compare_list=[])
    await send_compare_keyboard(c, state)

async def send_compare_keyboard(c: CallbackQuery, state: FSMContext):
    data = await state.get_data()
    chosen = set(data.get('compare_list', []))
    names = list(WorkWithDB.load_all().keys())
    buttons = []
    for n in names:
        prefix = '‚úÖ ' if n in chosen else '‚ñ´Ô∏è '
        buttons.append(InlineKeyboardButton(
            text=f"{prefix}{n}", callback_data=f"toggle:{n}"
        ))
    buttons.append(InlineKeyboardButton(
        text="üîÄ –°—Ä–∞–≤–Ω–∏—Ç—å", callback_data="run_compare"
    ))
    kb = InlineKeyboardMarkup(inline_keyboard=[buttons[i:i+2] for i in range(0, len(buttons), 2)])
    try:
        await c.message.edit_text("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–æ—Ç–º–µ—Ç—å—Ç–µ –≥–∞–ª–æ—á–∫–æ–π):", reply_markup=kb)
    except:
        await c.message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–æ—Ç–º–µ—Ç—å—Ç–µ –≥–∞–ª–æ—á–∫–æ–π):", reply_markup=kb)

@router.callback_query(F.data.startswith('toggle:'))
async def toggle_model(c: CallbackQuery, state: FSMContext):
    data = await state.get_data()
    chosen = set(data.get('compare_list', []))
    model = c.data.split(':',1)[1]
    if model in chosen:
        chosen.remove(model)
    else:
        chosen.add(model)
    await state.update_data(compare_list=list(chosen))
    await send_compare_keyboard(c, state)

@router.callback_query(F.data == 'run_compare')
async def run_compare(c: CallbackQuery, state: FSMContext):
    data = await state.get_data()
    chosen = data.get('compare_list', [])
    if len(chosen) < 2:
        return await c.message.answer("–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤—ã–±–µ—Ä–∏—Ç–µ –∫–∞–∫ –º–∏–Ω–∏–º—É–º –¥–≤–µ –º–æ–¥–µ–ª–∏.")
    db = WorkWithDB.load_all()
    pairs = []
    for name in chosen:
        specs = json.dumps(db[name], ensure_ascii=False)
        pairs.append(f"{name}: {specs}")
    content = "; ".join(pairs)
    msg = [{'role':'user', 'content': f"–°—Ä–∞–≤–Ω–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ VTOL-–¥—Ä–æ–Ω—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º: {content}"}]
    resp = ollama.chat(model='deepseek-r1:8b', messages=msg)
    report = resp['message']['content']
    lang = chat_lang.get(c.message.chat.id, 'ru')
    await c.message.answer(report)
    audio = await WorkWithTTS.text_to_speech(task='compare', text=report, lang=lang)
    await c.message.answer_audio(BufferedInputFile(audio, filename='compare.mp3'))
    await state.clear()
