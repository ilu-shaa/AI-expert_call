import json
import os
import tempfile
import re
import whisper
import ollama
from aiogram import Router, F
from aiogram.filters import Command
from aiogram.types import (
    Message, CallbackQuery, BufferedInputFile,
    InlineKeyboardMarkup, InlineKeyboardButton
)
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import StatesGroup, State

from app.config import OPENROUTER_API_KEY
from app.keyboards.start_keyboard import lang_menu, start_kb, back_to_start
from app.static_files.bot_answers import GREETINGS, CERTIFICATE
from app.new_voice_handler import chat_lang, WHISPER_LANG
from app.workTools.WorkWithDB import WorkWithDB
from app.workTools.WorkWithTTS import WorkWithTTS
from app.workTools.WorkWithLLM import MistralAPI
from app.workTools.WorkWithCache import WorkWithCache
from app.workTools.search_db import search_db

# FSM states
class Flag(StatesGroup):
    awaiting_question = State()
    awaiting_tts_text = State()
    awaiting_compare_selection = State()

router = Router()
_whisper_model = None

def get_whisper_model():
    global _whisper_model
    if _whisper_model is None:
        _whisper_model = whisper.load_model('tiny')
    return _whisper_model

# -------------------
# /start and language selection
# -------------------
@router.message(Command('start'))
async def cmd_start(msg: Message):
    chat_lang.pop(msg.chat.id, None)
    await msg.answer(GREETINGS, reply_markup=lang_menu)

@router.callback_query(F.data.startswith('set_lang:'))
async def set_lang(c: CallbackQuery):
    lang = c.data.split(':', 1)[1]
    chat_lang[c.message.chat.id] = lang
    confirm = {'ru': '‚úÖ –†—É—Å—Å–∫–∏–π', 'en': '‚úÖ English', 'cn': '‚úÖ ‰∏≠Êñá'}[lang]
    await c.message.edit_text(confirm, reply_markup=await start_kb(lang))

# -------------------
# –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è / –ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è
# -------------------
@router.callback_query(F.data == 'performance')
@router.callback_query(F.data.startswith('presentaion_'))
async def show_intro(c: CallbackQuery):
    lang = chat_lang.get(c.message.chat.id, 'ru')
    key = f"{c.data}_{lang}"
    if WorkWithCache.check_key(key):
        audio_bytes, text = WorkWithCache.get_cache(key)
    else:
        drone = c.data.split('_', 1)[1] if c.data != 'performance' else ''
        template = {
            'ru': f"–°–æ–∫—Ä–∞—Ç–∏—Ç–µ –¥–æ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –æ–ø–∏—Å–∞–Ω–∏–µ VTOL-–¥—Ä–æ–Ω–æ–≤ {drone}",
            'en': f"Summarize in 2 sentences a description of VTOL drones {drone}",
            'cn': f"Áî®2Âè•ËØùÁÆÄË¶ÅÊèèËø∞VTOLÊó†‰∫∫Êú∫ {drone}"
        }[lang]
        text = await MistralAPI.query(prompt=template, system=template, max_tokens=100)
        audio_bytes = await WorkWithTTS.text_to_speech(task=key, text=text, lang=lang)
        WorkWithCache.append_cache(key, audio_bytes, text)
    await c.message.answer(text)
    await c.message.answer_audio(BufferedInputFile(audio_bytes, filename='intro.mp3'))
# -------------------
# –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: —Å–ø–∏—Å–æ–∫ –¥—Ä–æ–Ω–æ–≤ –∏ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
# -------------------
@router.callback_query(F.data == 'features')
async def features_list(c: CallbackQuery):
    names = list(WorkWithDB.load_all().keys())
    buttons = [InlineKeyboardButton(text=n, callback_data=f"feat:{n}") for n in names]
    kb = InlineKeyboardMarkup(inline_keyboard=[buttons[i:i+2] for i in range(0, len(buttons), 2)])
    await c.message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫:", reply_markup=kb)

@router.callback_query(F.data.startswith('feat:'))
async def show_features(c: CallbackQuery):
    name = c.data.split(':',1)[1]
    specs = WorkWithDB.show_characteristics(name)
    lines = [f"üìå <b>{name}</b>"]
    for section in ("performance", "weights", "dimensions"):
        data = specs.get(section, {})
        if data:
            lines.append(f"\n<b>{section.title()}:</b>")
            for k,v in data.items():
                lines.append(f"‚Ä¢ {k}: {v}")
    docs = specs.get("compliance_documents", [])
    if docs:
        lines.append("\n<b>–î–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è:</b>")
        for d in docs:
            lines.append(f"‚Ä¢ {d}")

    await c.message.answer("\n".join(lines), parse_mode="HTML")

# -------------------
# –í—Ö–æ–¥ –≤ Q&A
# -------------------
@router.callback_query(F.data == 'question')
async def enter_qa(c: CallbackQuery, state: FSMContext):
    await state.set_state(Flag.awaiting_question)
    lang = chat_lang.get(c.message.chat.id, 'ru')
    prompt_text = {
        'ru': '‚ùì –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.',
        'en': '‚ùì Ask your question by text or send a voice message.',
        'cn': '‚ùì ËØ∑‰ª•ÊñáÂ≠óÊèêÈóÆÊàñÂèëÈÄÅËØ≠Èü≥Ê∂àÊÅØ„ÄÇ'
    }[lang]
    await c.message.answer(prompt_text)

@router.message(Flag.awaiting_question)
async def handle_question(m: Message, state: FSMContext):
    await state.clear()
    lang = chat_lang.get(m.chat.id, 'ru')

    if m.voice:
        with tempfile.NamedTemporaryFile(suffix=".ogg", delete=False) as tmp:
            await m.bot.download(m.voice.file_id, tmp.name)
            audio_path = tmp.name
        try:
            res = get_whisper_model().transcribe(audio_path, language=WHISPER_LANG.get(lang, 'en'))
            user_question = res.get('text', '').strip()
        except:
            user_question = ''
        finally:
            try: os.remove(audio_path)
            except: pass

        if not user_question:
            return await m.answer({
                'ru': "‚ùóÔ∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å.",
                'en': "‚ùóÔ∏è Could not transcribe audio.",
                'cn': "‚ùóÔ∏è Êó†Ê≥ïËØÜÂà´ËØ≠Èü≥„ÄÇ"
            }[lang])
    else:
        user_question = m.text or ""

    relevant = search_db(user_question, top_k=2)
    context = "\n\n".join([f"{name}:\n{info}" for name, info in relevant])
    system_prompt = {
        'ru': "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ VTOL-–¥—Ä–æ–Ω–∞–º. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞.",
        'en': "You are a VTOL drone expert. Use only the provided context to answer.",
        'cn': "ÊÇ®ÊòØ VTOL Êó†‰∫∫Êú∫‰∏ìÂÆ∂„ÄÇ‰ªÖ‰ΩøÁî®‰ª•‰∏ã‰ø°ÊÅØËøõË°åÂõûÁ≠î„ÄÇ"
    }[lang]

    prompt = f"{system_prompt}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {user_question}"

    answer = await MistralAPI.query(prompt=prompt, system=system_prompt, max_tokens=200)
    answer = answer.strip()
    if len(answer) > 1000:
        answer = answer[:997] + "..."

    await m.answer(answer)
    audio = await WorkWithTTS.text_to_speech(task="answer-question", text=answer, lang=lang)
    await m.answer_audio(BufferedInputFile(audio, filename="answer.mp3"))
# -------------------
# Comparator multi-select
# -------------------
@router.callback_query(F.data == 'compare')
async def ask_compare(c: CallbackQuery, state: FSMContext):
    await state.set_state(Flag.awaiting_compare_selection)
    await state.update_data(compare_list=[])
    await send_compare_keyboard(c, state)

async def send_compare_keyboard(c: CallbackQuery, state: FSMContext):
    data = await state.get_data()
    chosen = set(data.get('compare_list', []))
    names = list(WorkWithDB.load_all().keys())
    buttons = []
    for n in names:
        mark = '‚úÖ' if n in chosen else '‚ñ´Ô∏è'
        buttons.append(InlineKeyboardButton(text=f"{mark} {n}", callback_data=f"toggle:{n}"))
    buttons.append(InlineKeyboardButton(text="üîÄ –°—Ä–∞–≤–Ω–∏—Ç—å", callback_data="run_compare"))
    kb = InlineKeyboardMarkup(inline_keyboard=[buttons[i:i+2] for i in range(0, len(buttons), 2)])
    try:
        await c.message.edit_text('–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:', reply_markup=kb)
    except:
        await c.message.answer('–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:', reply_markup=kb)

@router.callback_query(F.data.startswith('toggle:'))
async def toggle_model(c: CallbackQuery, state: FSMContext):
    data = await state.get_data()
    chosen = set(data.get('compare_list', []))
    m = c.data.split(':', 1)[1]
    if m in chosen:
        chosen.remove(m)
    else:
        chosen.add(m)
    await state.update_data(compare_list=list(chosen))
    await send_compare_keyboard(c, state)

@router.callback_query(F.data == 'run_compare')
async def run_compare(c: CallbackQuery, state: FSMContext):
    data = await state.get_data()
    chosen = data.get('compare_list', [])
    if len(chosen) < 2:
        return await c.message.answer('–í—ã–±–µ—Ä–∏—Ç–µ –º–∏–Ω–∏–º—É–º 2 –¥—Ä–æ–Ω–∞.')
    db = WorkWithDB.load_all()
    pairs = [f"{n}: {json.dumps(db[n], ensure_ascii=False)}" for n in chosen]
    content = ' ; '.join(pairs)
    lang = chat_lang.get(c.message.chat.id, 'ru')
    system_msg = {
        'ru': '–í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç. –û—Ç–≤–µ—Ç—å—Ç–µ –æ—á–µ–Ω—å –∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –±–µ–∑ —Ç–µ–≥–æ–≤.',
        'en': 'You are an expert. Answer very concisely in English without tags.',
        'cn': 'ÊÇ®ÊòØÂ∞àÂÆ∂ÔºåË´ãÈùûÂ∏∏Á∞°Áü≠Âú∞ÂõûÁ≠îÔºå‰∏çË¶Å‰ΩøÁî®Ê®ôÁ±§„ÄÇ'
    }[lang]
    user_msg = f"–°—Ä–∞–≤–Ω–∏—Ç–µ —ç—Ç–∏ –¥—Ä–æ–Ω—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º: {content}"
    try:
        resp = ollama.chat(
            model='qwen3:8b',
            messages=[
                {'role': 'system', 'content': system_msg},
                {'role': 'user', 'content': user_msg}
            ]
        )
        report = resp['message']['content']
    except Exception as e:
        report = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏: {e}"
    # –£–¥–∞–ª—è–µ–º –ª—é–±—ã–µ —Ç–µ–≥–∏
    report = re.sub(r'<[^>]+>', '', report).strip()
    await c.message.answer(report, parse_mode=None)
    audio = await WorkWithTTS.text_to_speech(task='compare', text=report, lang=lang)
    await c.message.answer_audio(BufferedInputFile(audio, filename='compare.mp3'))
    await state.clear()