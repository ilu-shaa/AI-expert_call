import json
import os
import tempfile
import re
import whisper
import ollama
from aiogram import Router, F
from aiogram.filters import Command
from aiogram.types import (
    Message, CallbackQuery, BufferedInputFile,
    InlineKeyboardMarkup, InlineKeyboardButton
)
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import StatesGroup, State

from app.config import OPENROUTER_API_KEY
from app.keyboards.start_keyboard import lang_menu, start_kb, back_to_start
from app.static_files.bot_answers import GREETINGS, CERTIFICATE
from app.new_voice_handler import chat_lang, WHISPER_LANG
from app.workTools.WorkWithDB import WorkWithDB
from app.workTools.WorkWithTTS import WorkWithTTS
from app.workTools.WorkWithLLM import MistralAPI
from app.workTools.WorkWithCache import WorkWithCache
from app.workTools.search_db import search_db


class Flag(StatesGroup):
    awaiting_question = State()
    awaiting_tts_text = State()
    awaiting_compare_selection = State()

router = Router()
_whisper_model = None

def get_whisper_model():
    global _whisper_model
    if _whisper_model is None:
        _whisper_model = whisper.load_model('tiny')
    return _whisper_model

# -------------------
# /start
# -------------------
@router.message(Command('start'))
async def cmd_start(msg: Message):
    chat_lang.pop(msg.chat.id, None)
    await msg.answer(GREETINGS, reply_markup=lang_menu)

@router.callback_query(F.data.startswith('set_lang:'))
async def set_lang(c: CallbackQuery):
    lang = c.data.split(':', 1)[1]
    chat_lang[c.message.chat.id] = lang
    confirm = {'ru': 'âœ… Ğ ÑƒÑÑĞºĞ¸Ğ¹', 'en': 'âœ… English', 'cn': 'âœ… ä¸­æ–‡'}[lang]
    await c.message.edit_text(confirm, reply_markup=await start_kb(lang))

# -------------------
# ĞŸÑ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ / ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑ‚Ğ°Ñ†Ğ¸Ñ
# -------------------
@router.callback_query(F.data == 'performance')
@router.callback_query(F.data.startswith('presentaion_'))
async def show_intro(c: CallbackQuery):
    lang = chat_lang.get(c.message.chat.id, 'ru')
    key = f"{c.data}_{lang}"
    if WorkWithCache.check_key(key):
        audio_bytes, text = WorkWithCache.get_cache(key)
    else:
        drone = c.data.split('_', 1)[1] if c.data != 'performance' else ''
        template = {
            'ru': f"Ğ¡Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚Ğµ Ğ´Ğ¾ 2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ VTOL-Ğ´Ñ€Ğ¾Ğ½Ğ¾Ğ² {drone}",
            'en': f"Summarize in 2 sentences a description of VTOL drones {drone}",
            'cn': f"ç”¨2å¥è¯ç®€è¦æè¿°VTOLæ— äººæœº {drone}"
        }[lang]
        text = await MistralAPI.query(prompt=template, system=template, max_tokens=100)
        audio_bytes = await WorkWithTTS.text_to_speech(task=key, text=text, lang=lang)
        WorkWithCache.append_cache(key, audio_bytes, text)
    await c.message.answer(text)
    await c.message.answer_audio(BufferedInputFile(audio_bytes, filename='intro.mp3'))
# -------------------
# Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸: ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
# -------------------
@router.callback_query(F.data == 'features')
async def features_list(c: CallbackQuery):
    lang = chat_lang.get(c.message.chat.id, 'ru')
    prompt = {'ru': 'Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ...', 'en': 'Select a model...', 'cn': 'è¯·é€‰æ‹©å‹å·...'}[lang]
    names = list(WorkWithDB.load_all().keys())
    buttons = [InlineKeyboardButton(text=n, callback_data=f'feat:{n}') for n in names]
    kb = InlineKeyboardMarkup(inline_keyboard=[buttons[i:i+2] for i in range(0, len(buttons), 2)])
    await c.message.answer(prompt, reply_markup=kb)

@router.callback_query(F.data.startswith('feat:'))
async def show_features(c: CallbackQuery):
    name = c.data.split(':', 1)[1]
    specs = WorkWithDB.show_characteristics(name)
    lines = [f"ğŸ“Œ {name}"]
    for section in ('performance', 'weights', 'dimensions'):
        data = specs.get(section, {})
        if data:
            lines.append(f"\n{section.title()}:")
            for k, v in data.items():
                lines.append(f'â€¢ {k}: {v}')
    await c.message.answer("\n".join(lines))

# -------------------
# Ğ’Ñ…Ğ¾Ğ´ Ğ² Q&A
# -------------------
@router.callback_query(F.data == 'question')
async def enter_qa(c: CallbackQuery, state: FSMContext):
    await state.set_state(Flag.awaiting_question)
    lang = chat_lang.get(c.message.chat.id, 'ru')
    prompt_text = {
        'ru': 'â“ Ğ—Ğ°Ğ´Ğ°Ğ¹Ñ‚Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ÑŒÑ‚Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ.',
        'en': 'â“ Ask your question by text or send a voice message.',
        'cn': 'â“ è¯·ä»¥æ–‡å­—æé—®æˆ–å‘é€è¯­éŸ³æ¶ˆæ¯ã€‚'
    }[lang]
    await c.message.answer(prompt_text)

@router.message(Flag.awaiting_question)
async def handle_question(m: Message, state: FSMContext):
    await state.clear()
    lang = chat_lang.get(m.chat.id, 'ru')

    if m.voice:
        with tempfile.NamedTemporaryFile(suffix=".ogg", delete=False) as tmp:
            await m.bot.download(m.voice.file_id, tmp.name)
            audio_path = tmp.name
        try:
            res = get_whisper_model().transcribe(audio_path, language=WHISPER_LANG.get(lang, 'en'))
            user_question = res.get('text', '').strip()
        except:
            user_question = ''
        finally:
            try: os.remove(audio_path)
            except: pass

        if not user_question:
            return await m.answer({
                'ru': "â—ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ñ‚ÑŒ Ñ€ĞµÑ‡ÑŒ.",
                'en': "â—ï¸ Could not transcribe audio.",
                'cn': "â—ï¸ æ— æ³•è¯†åˆ«è¯­éŸ³ã€‚"
            }[lang])
    else:
        user_question = m.text or ""

    relevant = search_db(user_question, top_k=2)
    context = "\n\n".join([f"{name}:\n{info}" for name, info in relevant])
    system_prompt = {
        'ru': "Ğ¢Ñ‹ â€” ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ VTOL-Ğ´Ñ€Ğ¾Ğ½Ğ°Ğ¼. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´Ñ‘Ğ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°.",
        'en': "You are a VTOL drone expert. Use only the provided context to answer.",
        'cn': "æ‚¨æ˜¯ VTOL æ— äººæœºä¸“å®¶ã€‚ä»…ä½¿ç”¨ä»¥ä¸‹ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚"
    }[lang]

    prompt = f"{system_prompt}\n\nĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚:\n{context}\n\nĞ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {user_question}"

    answer = await MistralAPI.query(prompt=prompt, system=system_prompt, max_tokens=200)
    answer = answer.strip()
    if len(answer) > 1000:
        answer = answer[:997] + "..."

    await m.answer(answer)
    audio = await WorkWithTTS.text_to_speech(task="answer-question", text=answer, lang=lang)
    await m.answer_audio(BufferedInputFile(audio, filename="answer.mp3"))

# -------------------
# ĞšĞ¾Ğ¼Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ¾Ñ€
# -------------------

@router.callback_query(F.data == 'compare')
async def ask_compare(c: CallbackQuery, state: FSMContext):
    await state.set_state(Flag.awaiting_compare_selection)
    lang = chat_lang.get(c.message.chat.id, 'ru')
    await state.update_data(compare_list=[], lang=lang)
    await send_compare_keyboard(c, state)

async def send_compare_keyboard(c: CallbackQuery, state: FSMContext):
    data = await state.get_data()
    chosen = set(data.get('compare_list', []))
    lang = data.get('lang', 'ru')
    labels = {'ru': 'Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ÑŒ', 'en': 'Compare', 'cn': 'æ¯”è¾ƒ'}
    prompt = {'ru': 'Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:', 'en': 'Select drones:', 'cn': 'é€‰æ‹©æ— äººæœº:'}[lang]
    names = list(WorkWithDB.load_all().keys())
    buttons = [InlineKeyboardButton(text=('âœ… ' if n in chosen else 'â–«ï¸ ')+n, callback_data=f"toggle:{n}") for n in names]
    buttons.append(InlineKeyboardButton(text=f"ğŸ”€ {labels[lang]}", callback_data='run_compare'))
    kb = InlineKeyboardMarkup(inline_keyboard=[buttons[i:i+2] for i in range(0, len(buttons), 2)])
    try:
        await c.message.edit_text(prompt, reply_markup=kb)
    except:
        await c.message.answer(prompt, reply_markup=kb)

@router.callback_query(F.data.startswith('toggle:'))
async def toggle_model(c: CallbackQuery, state: FSMContext):
    data = await state.get_data()
    chosen = set(data['compare_list'])
    model = c.data.split(':',1)[1]
    chosen.symmetric_difference_update({model})
    await state.update_data(compare_list=list(chosen))
    await send_compare_keyboard(c, state)

@router.callback_query(F.data == 'run_compare')
async def run_compare(c: CallbackQuery, state: FSMContext):
    data = await state.get_data()
    chosen = data.get('compare_list', [])
    if len(chosen) < 2:
        return await c.message.answer('Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 2 Ğ´Ñ€Ğ¾Ğ½Ğ°.')
    db = WorkWithDB.load_all()
    pairs = [f"{n}: {json.dumps(db[n], ensure_ascii=False)}" for n in chosen]
    content = ' ; '.join(pairs)
    lang = chat_lang.get(c.message.chat.id, 'ru')
    system_msg = {
        'ru': 'Ğ’Ñ‹ â€” ÑĞºÑĞ¿ĞµÑ€Ñ‚. ĞÑ‚Ğ²ĞµÑ‚ÑŒÑ‚Ğµ Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ Ğ±ĞµĞ· Ñ‚ĞµĞ³Ğ¾Ğ².',
        'en': 'You are an expert. Answer very concisely in English without tags.',
        'cn': 'æ‚¨æ˜¯å°ˆå®¶ï¼Œè«‹éå¸¸ç°¡çŸ­åœ°å›ç­”ï¼Œä¸è¦ä½¿ç”¨æ¨™ç±¤ã€‚'
    }[lang]
    user_msg = {
    'ru': f"Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ñ‚Ğµ ÑÑ‚Ğ¸ Ğ´Ñ€Ğ¾Ğ½Ñ‹ Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼: {content}",
    'en': f"Compare these drones by key parameters in English: {content}",
    'cn': f"è¯·ç”¨ä¸­æ–‡æ¯”è¾ƒä»¥ä¸‹æ— äººæœºçš„å…³é”®å‚æ•°: {content}"}[lang]
    try:
        resp = ollama.chat(
            model='qwen3:8b',
            messages=[
                {'role': 'system', 'content': system_msg},
                {'role': 'user', 'content': user_msg}
            ]
        )
        report = resp['message']['content']
    except Exception as e:
        report = f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸: {e}"
    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ»ÑĞ±Ñ‹Ğµ Ñ‚ĞµĞ³Ğ¸
    report = re.sub(r'<[^>]+>', '', report).strip()
    await c.message.answer(report, parse_mode=None)
    audio = await WorkWithTTS.text_to_speech(task='compare', text=report, lang=lang)
    await c.message.answer_audio(BufferedInputFile(audio, filename='compare.mp3'))
    await state.clear()

